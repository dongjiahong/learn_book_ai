"""
Integration test for the complete answer evaluation system
"""

import asyncio
import sys
import os
from pathlib import Path

# Add the backend directory to the Python path
backend_dir = Path(__file__).parent
sys.path.insert(0, str(backend_dir))

from app.models.database import get_db, engine
from app.models.models import Base, User, KnowledgeBase, Document, Question
from app.services.evaluation_service import evaluation_service
from app.api.evaluation import router as evaluation_router
from sqlalchemy.orm import sessionmaker
from fastapi.testclient import TestClient
from fastapi import FastAPI

# Create test database session
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


async def setup_test_data():
    """Set up comprehensive test data"""
    print("Setting up comprehensive test data...")
    
    # Create tables
    Base.metadata.create_all(bind=engine)
    
    db = TestingSessionLocal()
    try:
        # Create test user
        test_user = User(
            username="integration_test_user",
            email="integration@example.com",
            password_hash="hashed_password"
        )
        db.add(test_user)
        db.commit()
        db.refresh(test_user)
        
        # Create test knowledge base
        test_kb = KnowledgeBase(
            user_id=test_user.id,
            name="Integration Test KB",
            description="Knowledge base for integration testing"
        )
        db.add(test_kb)
        db.commit()
        db.refresh(test_kb)
        
        # Create test document
        test_doc = Document(
            knowledge_base_id=test_kb.id,
            filename="integration_test.txt",
            file_path="/test/integration_test.txt",
            file_type="txt",
            file_size=2000,
            processed=True
        )
        db.add(test_doc)
        db.commit()
        db.refresh(test_doc)
        
        # Create multiple test questions
        test_questions = [
            Question(
                document_id=test_doc.id,
                question_text="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                context="äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
                difficulty_level=2
            ),
            Question(
                document_id=test_doc.id,
                question_text="æœºå™¨å­¦ä¹ çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
                context="æœºå™¨å­¦ä¹ ä¸»è¦åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»å‹ã€‚",
                difficulty_level=3
            ),
            Question(
                document_id=test_doc.id,
                question_text="æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
                context="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚",
                difficulty_level=4
            )
        ]
        
        for question in test_questions:
            db.add(question)
        
        db.commit()
        
        # Refresh questions to get IDs
        for question in test_questions:
            db.refresh(question)
        
        print(f"Created test user: {test_user.id}")
        print(f"Created test knowledge base: {test_kb.id}")
        print(f"Created test document: {test_doc.id}")
        print(f"Created {len(test_questions)} test questions")
        
        return test_user.id, [q.id for q in test_questions]
        
    except Exception as e:
        print(f"Error setting up test data: {e}")
        db.rollback()
        return None, []
    finally:
        db.close()


async def test_evaluation_service():
    """Test the evaluation service with different answer qualities"""
    print("\n=== Testing Evaluation Service ===")
    
    user_id, question_ids = await setup_test_data()
    if not user_id or not question_ids:
        print("Failed to setup test data")
        return False
    
    db = TestingSessionLocal()
    try:
        # Test different answer qualities for the first question
        test_cases = [
            {
                "name": "ä¼˜ç§€ç­”æ¡ˆ",
                "answer": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿã€æ‰©å±•å’Œè¾…åŠ©äººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚AIç³»ç»Ÿèƒ½å¤Ÿæ‰§è¡Œæ„ŸçŸ¥ã€å­¦ä¹ ã€æ¨ç†ã€å†³ç­–ç­‰é€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„å¤æ‚ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¸“å®¶ç³»ç»Ÿç­‰åº”ç”¨é¢†åŸŸã€‚",
                "expected_score_range": (8, 10)
            },
            {
                "name": "è‰¯å¥½ç­”æ¡ˆ",
                "answer": "äººå·¥æ™ºèƒ½æ˜¯è®©è®¡ç®—æœºèƒ½å¤Ÿåƒäººä¸€æ ·æ€è€ƒå’Œå­¦ä¹ çš„æŠ€æœ¯ï¼Œå¯ä»¥ç”¨æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚",
                "expected_score_range": (6, 8)
            },
            {
                "name": "åŸºç¡€ç­”æ¡ˆ",
                "answer": "äººå·¥æ™ºèƒ½å°±æ˜¯è®©æœºå™¨å˜èªæ˜çš„æŠ€æœ¯ã€‚",
                "expected_score_range": (3, 6)
            },
            {
                "name": "ä¸å®Œæ•´ç­”æ¡ˆ",
                "answer": "AI",
                "expected_score_range": (0, 3)
            }
        ]
        
        results = []
        for i, test_case in enumerate(test_cases):
            print(f"\næµ‹è¯•æ¡ˆä¾‹ {i+1}: {test_case['name']}")
            print(f"ç­”æ¡ˆ: {test_case['answer']}")
            
            # Evaluate answer
            result = await evaluation_service.evaluate_answer(
                db=db,
                user_id=user_id,
                question_id=question_ids[0],
                user_answer=test_case['answer'],
                save_record=True
            )
            
            if result.get('success'):
                evaluation = result['evaluation']
                score = evaluation.get('score', 0)
                
                print(f"è¯„åˆ†: {score}/10")
                print(f"åé¦ˆ: {evaluation.get('feedback', 'N/A')[:100]}...")
                
                # Check if score is in expected range
                min_score, max_score = test_case['expected_score_range']
                if min_score <= score <= max_score:
                    print(f"âœ“ è¯„åˆ†åœ¨é¢„æœŸèŒƒå›´å†… ({min_score}-{max_score})")
                    results.append(True)
                else:
                    print(f"âœ— è¯„åˆ†è¶…å‡ºé¢„æœŸèŒƒå›´ ({min_score}-{max_score})")
                    results.append(False)
                
                # Check if detailed analysis exists
                if evaluation.get('detailed_analysis'):
                    print("âœ“ åŒ…å«è¯¦ç»†åˆ†æ")
                else:
                    print("âœ— ç¼ºå°‘è¯¦ç»†åˆ†æ")
                
            else:
                print(f"âœ— è¯„ä¼°å¤±è´¥: {result.get('error', 'Unknown error')}")
                results.append(False)
            
            print("-" * 60)
        
        success_rate = sum(results) / len(results) * 100
        print(f"\nè¯„ä¼°æœåŠ¡æµ‹è¯•æˆåŠŸç‡: {success_rate:.1f}%")
        
        return success_rate >= 75  # 75% success rate threshold
        
    except Exception as e:
        print(f"Error during evaluation testing: {e}")
        return False
    finally:
        db.close()


async def test_evaluation_statistics():
    """Test evaluation statistics functionality"""
    print("\n=== Testing Evaluation Statistics ===")
    
    db = TestingSessionLocal()
    try:
        # Get test user
        user = db.query(User).filter(User.username == "integration_test_user").first()
        if not user:
            print("No test user found")
            return False
        
        # Get statistics
        stats = await evaluation_service.get_evaluation_statistics(
            db=db,
            user_id=user.id
        )
        
        print(f"æ€»ç­”é¢˜æ•°: {stats['total_answers']}")
        print(f"å¹³å‡åˆ†: {stats['average_score']}")
        print(f"åˆ†æ•°åˆ†å¸ƒ: {stats['score_distribution']}")
        print(f"æœ€è¿‘è¡¨ç°è®°å½•æ•°: {len(stats['recent_performance'])}")
        
        # Validate statistics
        if stats['total_answers'] > 0:
            print("âœ“ ç»Ÿè®¡æ•°æ®æ­£å¸¸")
            return True
        else:
            print("âœ— ç»Ÿè®¡æ•°æ®å¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"Error testing statistics: {e}")
        return False
    finally:
        db.close()


async def test_answer_records():
    """Test answer records functionality"""
    print("\n=== Testing Answer Records ===")
    
    db = TestingSessionLocal()
    try:
        # Get test user
        user = db.query(User).filter(User.username == "integration_test_user").first()
        if not user:
            print("No test user found")
            return False
        
        # Get answer records
        records = await evaluation_service.get_answer_records(
            db=db,
            user_id=user.id,
            skip=0,
            limit=10
        )
        
        print(f"æ‰¾åˆ° {len(records)} æ¡ç­”é¢˜è®°å½•")
        
        if records:
            # Show sample record
            sample = records[0]
            print(f"ç¤ºä¾‹è®°å½•:")
            print(f"  é—®é¢˜: {sample['question_text'][:50]}...")
            print(f"  ç”¨æˆ·ç­”æ¡ˆ: {sample['user_answer'][:50]}...")
            print(f"  è¯„åˆ†: {sample['score']}")
            print(f"  æ—¶é—´: {sample['answered_at']}")
            print("âœ“ ç­”é¢˜è®°å½•åŠŸèƒ½æ­£å¸¸")
            return True
        else:
            print("âœ— æœªæ‰¾åˆ°ç­”é¢˜è®°å½•")
            return False
            
    except Exception as e:
        print(f"Error testing answer records: {e}")
        return False
    finally:
        db.close()


async def main():
    """Main integration test function"""
    print("Starting Answer Evaluation System Integration Test")
    print("=" * 60)
    
    try:
        # Run all tests
        test_results = []
        
        # Test evaluation service
        result1 = await test_evaluation_service()
        test_results.append(("Evaluation Service", result1))
        
        # Test statistics
        result2 = await test_evaluation_statistics()
        test_results.append(("Evaluation Statistics", result2))
        
        # Test answer records
        result3 = await test_answer_records()
        test_results.append(("Answer Records", result3))
        
        # Summary
        print("\n" + "=" * 60)
        print("Integration Test Results:")
        print("-" * 60)
        
        passed = 0
        for test_name, result in test_results:
            status = "âœ“ PASS" if result else "âœ— FAIL"
            print(f"{test_name:<25} {status}")
            if result:
                passed += 1
        
        print("-" * 60)
        print(f"Overall: {passed}/{len(test_results)} tests passed")
        
        if passed == len(test_results):
            print("ğŸ‰ All integration tests passed!")
            print("Answer Evaluation System is working correctly!")
        else:
            print("âš ï¸  Some tests failed. Please check the implementation.")
        
        return passed == len(test_results)
        
    except Exception as e:
        print(f"Integration test execution failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)